{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1600823059877
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2020-09-23T01:04:19.4876209Z",
       "execution_start_time": "2020-09-23T01:04:09.270819Z",
       "livy_statement_state": "available",
       "queued_time": "2020-09-23T01:03:17.9336417Z",
       "session_id": 6,
       "spark_pool": "MediumPool",
       "state": "finished",
       "statement_id": 3
      },
      "text/plain": [
       "StatementMeta(MediumPool, 6, 3, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "****************\n",
       "Python version: 3.6.1 |Continuum Analytics, Inc.| (default, May 11 2017, 13:09:58) \n",
       "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
       "Spark version: 2.4.4.2.6.99.201-22124361\n",
       "****************"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\r\n",
    "import pyspark\r\n",
    "import os\r\n",
    "import urllib\r\n",
    "import sys\r\n",
    "from datetime import datetime\r\n",
    "from datetime import datetime\r\n",
    "from dateutil import parser\r\n",
    "from pyspark.sql.functions import *\r\n",
    "from pyspark.ml.classification import *\r\n",
    "from pyspark.ml.evaluation import *\r\n",
    "from pyspark.ml.feature import *\r\n",
    "from pyspark.sql.types import StructType, StructField\r\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\r\n",
    "from azureml.core.run import Run\r\n",
    "\r\n",
    "# print runtime versions\r\n",
    "print('****************')\r\n",
    "print('Python version: {}'.format(sys.version))\r\n",
    "print('Spark version: {}'.format(spark.version))\r\n",
    "print('****************')\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1600823100893
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2020-09-23T01:05:00.5861945Z",
       "execution_start_time": "2020-09-23T01:04:58.5458275Z",
       "livy_statement_state": "available",
       "queued_time": "2020-09-23T01:04:58.5091107Z",
       "session_id": 6,
       "spark_pool": "MediumPool",
       "state": "finished",
       "statement_id": 4
      },
      "text/plain": [
       "StatementMeta(MediumPool, 6, 4, Finished, Available)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize logger\r\n",
    "run = Run.get_context()\r\n",
    "\r\n",
    "# start Spark session\r\n",
    "spark = pyspark.sql.SparkSession.builder.appName('NYCGreenTaxi')\\\r\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\") \\\r\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\r\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\r\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.opendatasets import NycTlcGreen\r\n",
    "\r\n",
    "end_date = parser.parse('2018-06-01')\r\n",
    "start_date = parser.parse('2018-05-01')\r\n",
    "nyc_green = NycTlcGreen(start_date=start_date, end_date=end_date)\r\n",
    "nyc_green_df = nyc_green.to_spark_dataframe()\r\n",
    "\r\n",
    "# Print schema of input data\r\n",
    "print(\"Schema of the input data:\")\r\n",
    "nyc_green_df.printSchema()\r\n",
    "\r\n",
    "# Print statistical summary for predicted Y value - total amount for trips\r\n",
    "print(\"Statistics summary for Total Amount:\")\r\n",
    "nyc_green_df.describe(\"totalAmount\").show()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
